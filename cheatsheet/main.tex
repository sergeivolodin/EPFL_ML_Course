\documentclass[]{article}
\usepackage{multicol}
\usepackage[a4paper, margin=0.5in]{geometry}
\usepackage{blindtext}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\newcommand{\myvector}[1]{{\bf{#1}}}
\newcommand{\x}{\myvector{x}}
\newcommand{\y}{\myvector{y}}
\newcommand{\w}{\myvector{w}}
\newcommand{\uu}{\myvector{u}}
\newcommand{\vv}{\myvector{v}}
\newcommand{\eqdef}{\overset{\mbox{\tiny{def}}}{=}}
\newcommand{\mytitle}[1]{ {\bf $\rightarrow$ \underline{#1}}\\}
\begin{document}
\begin{center}
	{\large Machine Learning Cheatsheet}\\
	by Sergei Volodin
\end{center}
\begin{multicols*}{2}
\mytitle{Introduction}
\fbox{Train $(x,y)$}+\fbox{Algo}$\Rightarrow$\fbox{Model}\\
\fbox{Test $x$}+\fbox{Model}$\Rightarrow$\fbox{Prediction}\\
Regression, Classification, Unsupervised

\mytitle{Linear Regression}
$\{(\x_n, y_n)\}_{i=1}^N$, $\x\in \mathbb{R}^D$.\\
Want: $\forall n\in\overline{1, N}\,f(\x_n)\approx y_n$\\
$\tilde{\x}_n=(1,x_{n1},...,x_{nD})$,\\
$f(\x_n)\eqdef \tilde{\x}_n^T\w$\\
Learning $=$ finding w,\\
$D>N$ $\Rightarrow$ need reg.

\mytitle{Cost functions}
$L(\w)=\mbox{MSE}(\w)=\frac{1}{2N}\sum\limits_{n=1}^N\left[y_n-f(\x_n)\right]^2$\\
$L(\w)=\mbox{MAE}(\w)=\frac{1}{N}\sum\limits_{n=1}^n\left|y_n-f(\x_n)\right|$\\

Tradeoff: computational (diff-ble) vs. statistical (robustness) properties

\mytitle{Convexity}
$f(\uu),\,\uu\in X$~--- {\em convex} $\Leftrightarrow$
$\forall \uu, \vv\in X\,\forall \lambda\in \left(0,1\right)$\\
$f(\lambda \uu + (1-\lambda) \vv)\leqslant \lambda f(\uu)+(1-\lambda)f(\vv)$\\
({\em strictly c.} $\Leftrightarrow$ $<$)
$f$~--- strictly convex $\Rightarrow$ $\exists !$ global inf.

\mytitle{Optimization}
Optimization problem:
$\inf\limits_{\w}L(\w)$. Grid search.\\
{\bf GD:} $O(N\cdot D)$, $\w^{(t+1)}=\w^{(t)}-\gamma\nabla L(\w^{(t)})$\\
$L(\w)=\frac{1}{N}\sum\limits_{n=1}^N L_n(\w)$\\
{\bf SGD:} $O(D)$, $\w^{(t+1)}=\w^{(t)}-\gamma\nabla L_n(\w^{(t)})$\\
$\mathbb{E}\nabla L_n=\nabla L$\\
{\bf Batch GD:}  $\w^{(t+1)}=\w^{(t)}-\frac{\gamma}{|B|} \sum\limits_{n\in B}\nabla L_n(\w^{(t)})$

$\forall\uu\,L(\uu)\geqslant L(\w)+g^T(\uu-\w)$ $\Leftrightarrow$\\
$g$~--- {\em subgradient} of $L$ at $\w$. $\exists \nabla L(\w)$ $\Leftrightarrow$
$\exists!$ subgradient $\nabla L(\w)$

Convex set. Projection. Proj. GD. Constrainted $\rightarrow$ unconstr. problem. Line search. Feature normalization. 

\mytitle{Least Squares}
Norm. eqs: $\nabla L(\w)=0=-\frac{1}{N}X^T(y-Xw)$
$\Rightarrow$ $\w^*=(X^TX)^{-1}X^T\myvector{y}$

\mytitle{Ill-conditioning}
$D>N$ no inverse\\
Columns of $X$ are $\approx$ collin. $\Rightarrow$ SVD

\mytitle{Max Likelihood}
(MSE) $y_n=\x_n^T\w+\varepsilon_n$, $\varepsilon_n\sim N(0, \sigma^2)$\\
$p(\y|X,\w)=\prod\limits_{n=1}^NN(y_n|\x_n^Tw,\sigma^2)$,\\
$L_{\mbox{\tiny LL MLE}}=\log p(\y|X,\w)\approx \mathbb{E}\log p(y|\x,\w)$ $\Rightarrow$\\
$\arg\max L_{\mbox{\tiny LL MLE}}=\arg\min L_{\mbox{\tiny MSE}}$

\mytitle{Overfitting}
Overfit: too complex model, fit the noise\\
Underfit: no good model in the set\\
Both bad. Polynomial basis example

Reg.: $\min\limits_\w L(\w)+\Omega(\w)$, last for complexity.\\
L2: $+\lambda \|\w\|_2^2$

\mytitle{Ridge Regression}
$\w^*=(X^TX+\lambda'I)^{-1}X^T\y$, $2N\lambda=\lambda'$

\mytitle{Lasso}
$\Omega(\w)=\lambda \|w\|_1=\lambda\sum |w_i|$ $\Rightarrow$ sparse solution

\mytitle{Cross-Validation}
\mytitle{Bias-Variance decomposition}
\mytitle{Classification}
\mytitle{Logistic Regression}
\mytitle{Generalized Linear Models}
\mytitle{K-Nearest Neighbor}
\mytitle{Support Vector Machines}
\mytitle{Kernel Regression}
\mytitle{Unsupervised Learning}
\mytitle{Gaussian Mixture Models}
\mytitle{EM algorithm}
\mytitle{Matrix Factorizations}
\mytitle{Text Representation Learning}
\mytitle{SVD and PCA}
\mytitle{NNs: Basics}
$x_j^{(l)}=\phi\left(\sum\limits_iw_{ij}^{(l)}x_i^{(l-1)}+b_j^{(l)}\right)$\\
Sigmoid: $\phi(x)=(1+\exp(-x))^{-1}$\\

\mytitle{NNs: Representation Power}
Approximation: Riemann integral (avg, stepwise, towers), Fourier transform smoothness

\mytitle{NNs: Backpropagation, Activation Functions}
$z^({l})=(W^{(l)})^Tx^{(l-1)}+b^{(l)}$, $\x^{(l)}=f^({l})(x^({l-1}))=\phi(z^{(l)})$\\
$\delta_j^{(l)}=\frac{\partial L_n}{\partial z_j^{(l)}}$, $\delta^{(l)}=W^{(l+1)}\delta^{(l+1)}\odot \phi'(z^{(l)})$\\
$\delta^{(L+1)}=-2(y_n-x^{(L+1)})\phi'(z^{(L+1)})$\\
$\frac{\partial L_n}{\partial w_{ij}^{(l)}}=\delta^{(l)}_jx_i^{(l-1)}$,
$\frac{\partial L_n}{\partial b_{j}^{(l)}}=\delta^{(l)}_j$.

Tanh: $\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$\\
ReLU: $(x)_+=\max\{0, x\}$\\
Leaky ReLU: $f(x)=\max\{\alpha x, x\}$
Maxout: $f(x)=\max\{x^Tw_1+b_1,...,x^Tw_k+b_k\}$

\mytitle{NNs: CNN}
Conv. 1D: $x^{(1)}[n]=\sum\limits_kf[k]x^{(0)}[n-k]$
Conv. 2D: $x^{(1)}[n,m]=\sum\limits_{k,l}f[k,l]x^{(0)}[n-k,m-l]$\\
Zero (add virtual zeros) and valid (reduce out size) padding\\
Channels.\\
FFT $\Rightarrow$ faster convolution sometimes\\

\mytitle{NNs: Regularization}
$\Omega(w)=\frac{1}{2}\sum\limits_{l=1}^{L+1}\mu^{(l)}\|W^{(l)}\|_F^2$. Weight decay.\\

\mytitle{NNs: Data Augmentation}
Image classif. $\Rightarrow$ shift, rotate, scale, ...\\

\mytitle{NNs: Dropout}
Keep node with $p\in(0,1)$. ``Simultaneous training'' of all subnetworks.

\mytitle{GMs: Bayes Nets}
Chain rule: $p(X_1,...,X_D)=p(X_1)p(X_2|X_1)...p(X_D|X_1,...,X_{D-1})$\\
Some dependencies can be missing.\\
Graph: $p(X_i|...X_j...)$ $\Leftrightarrow$ $X_j\to X_i$\\
$X\bot Y|Z$ $\Leftrightarrow$ $p(X, Y|Z)=p(X|Z)p(Y|Z)$. Can be sets\\
\begin{enumerate}
	\item $X, Y$ D-sep. by $Z$ $\Rightarrow$ $X\bot Y|Z$
	\item $X, Y$ not D-sep. by $Z$ $\Rightarrow$ $\exists p$, B.Net G$\colon$ $\neg X\bot Y|Z$
\end{enumerate}
$X,Y$ D-sep. by $Z$ $\Leftrightarrow$ $\forall x\in X,\,\forall y\in Y \forall \mbox{path} x\to y$ path is blocked by $Z$ $\Leftrightarrow$ $\exists u\in x\to y\colon$\\
$x\to y$ blocked by $Z$
\begin{enumerate}
	\item $u\in Z$, h-t-t or t-t-t
	\item $u$ h-t-h and neither $u$ nor its descendants $\in Z$
\end{enumerate}
$...\leftarrow X_i\rightarrow...$ tail-to-tail\\
$...\rightarrow X_i\leftarrow...$ head-to-head\\
$...\rightarrow X_i\rightarrow...$ head-to-tail\\
\mytitle{GMs: Factor Graphs}
$f(x_1,...,x_D)=\prod\limits_{a\in A}f_a(x_{\partial a})$\\
Graph: $V=\{f_a\}\cup\{x_i\}$. $(f,x)\in E\Leftrightarrow x\in x_{\partial a}$\\

\mytitle{GMs: Inference and Sum-Product Algorithm}
$f(z)=\sum\limits_{\sim z}f(z,...)$ (sum over all args but $z$)\\
$g(z)=\prod\limits_{i=1}^k\sum\limits_{\sim z}g_i(z,...)$\\
$\sum\limits_{\sim z}g_k(z,...)=\sum\limits_{\sim z}h(z,z_1,...,z_J)\prod\limits_{j=1}^J\sum\limits_{\sim z_j}h_j(z_j,...)$

\end{multicols*}
\end{document}
