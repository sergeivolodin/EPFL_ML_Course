\documentclass[]{article}
\usepackage{multicol}
\usepackage[a4paper, margin=0.5in]{geometry}
\usepackage{blindtext}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\newcommand{\myvector}[1]{{\bf{#1}}}
\newcommand{\x}{\myvector{x}}
\newcommand{\w}{\myvector{w}}
\newcommand{\uu}{\myvector{u}}
\newcommand{\vv}{\myvector{v}}
\newcommand{\eqdef}{\overset{\mbox{\tiny{def}}}{=}}
\newcommand{\mytitle}[1]{ {\bf $\rightarrow$ \underline{#1}}\\}
\begin{document}
\begin{center}
	{\large Machine Learning Cheatsheet}\\
	by Sergei Volodin
\end{center}
\begin{multicols*}{2}
\mytitle{Introduction}
\fbox{Train $(x,y)$}+\fbox{Algo}$\Rightarrow$\fbox{Model}\\
\fbox{Test $x$}+\fbox{Model}$\Rightarrow$\fbox{Prediction}\\
Regression, Classification, Unsupervised

\mytitle{Linear Regression}
$\{(\x_n, y_n)\}_{i=1}^N$, $\x\in \mathbb{R}^D$.\\
Want: $\forall n\in\overline{1, N}\,f(\x_n)\approx y_n$\\
$\tilde{\x}_n=(1,x_{n1},...,x_{nD})$,\\
$f(\x_n)\eqdef \tilde{\x}_n^T\w$\\
Learning $=$ finding w,\\
$D>N$ $\Rightarrow$ need reg.

\mytitle{Cost functions}
$L(\w)=\mbox{MSE}(\w)=\frac{1}{2N}\sum\limits_{n=1}^N\left[y_n-f(\x_n)\right]^2$\\
$L(\w)=\mbox{MAE}(\w)=\frac{1}{N}\sum\limits_{n=1}^n\left|y_n-f(\x_n)\right|$\\

Tradeoff: computational (diff-ble) vs. statistical (robustness) properties

\mytitle{Convexity}
$f(\uu),\,\uu\in X$~--- {\em convex} $\Leftrightarrow$
$\forall \uu, \vv\in X\,\forall \lambda\in \left(0,1\right)$\\
$f(\lambda \uu + (1-\lambda) \vv)\leqslant \lambda f(\uu)+(1-\lambda)f(\vv)$\\
({\em strictly c.} $\Leftrightarrow$ $<$)
$f$~--- strictly convex $\Rightarrow$ $\exists !$ global inf.

\mytitle{Optimization}
Optimization problem:
$$\inf\limits_{\w}L(\w)$$ Grid search.\\
{\bf GD:} $O(N\cdot D)$, $\w^{(t+1)}=\w^{(t)}-\gamma\nabla L(\w^{(t)})$\\
$L(\w)=\frac{1}{N}\sum\limits_{n=1}^N L_n(\w)$\\
{\bf SGD:} $O(D)$, $\w^{(t+1)}=\w^{(t)}-\gamma\nabla L_n(\w^{(t)})$\\
$\mathbb{E}\nabla L_n=\nabla L$\\
{\bf Batch GD:}  $\w^{(t+1)}=\w^{(t)}-\frac{\gamma}{|B|} \sum\limits_{n\in B}\nabla L_n(\w^{(t)})$

$\forall\uu\,L(\uu)\geqslant L(\w)+g^T(\uu-\w)$ $\Leftrightarrow$\\
$g$~--- {\em subgradient} of $L$ at $\w$. $\exists \nabla L(\w)$ $\Leftrightarrow$
$\exists!$ subgradient $\nabla L(\w)$

Convex set. Projection. Proj. GD. Constrainted $\rightarrow$ unconstr. problem. Line search. Feature normalization. 

\mytitle{Least Squares}
\mytitle{Ill-conditioning}
\mytitle{Max Likelihood}
\mytitle{Overfitting}
\mytitle{Ridge Regression}
\mytitle{Lasso}
\mytitle{Cross-Validation}
\mytitle{Bias-Variance decomposition}
\mytitle{Classification}
\mytitle{Logistic Regression}
\mytitle{Generalized Linear Models}
\mytitle{K-Nearest Neighbor}
\mytitle{Support Vector Machines}
\mytitle{Kernel Regression}
\mytitle{Unsupervised Learning}
\mytitle{Gaussian Mixture Models}
\mytitle{EM algorithm}
\mytitle{Matrix Factorizations}
\mytitle{Text Representation Learning}
\mytitle{SVD and PCA}
\mytitle{NNs: Basics}
\mytitle{NNs: Representation Power}
\mytitle{NNs: Backpropagation, Activation Functions}
\mytitle{NNs: CNN, Regularization, Data Augmentation, Dropout}
\mytitle{GMs: Bayes Nets}
\mytitle{GMs: Factor Graphs}
\mytitle{GMs: Inference and Sum-Product Algorithm}
\end{multicols*}
\end{document}
